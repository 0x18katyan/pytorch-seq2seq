{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/keon/seq2seq/\n",
    "https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
    "https://pytorch.org/docs/stable/nn.html#torch.nn.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.data/multi30k/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Multi30k.download('.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>')\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = TranslationDataset.splits(      \n",
    "  path = '.data/multi30k',  \n",
    "  exts = ['.de', '.en'],   \n",
    "  fields = [('src', SRC), ('trg', TRG)],\n",
    "  train = 'train', \n",
    "  validation = 'val', \n",
    "  test = 'test2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train.src, min_freq=2)\n",
    "TRG.build_vocab(train.trg, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train, valid, test), batch_size=BATCH_SIZE, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://discuss.pytorch.org/t/how-can-i-know-which-part-of-h-n-of-bidirectional-rnn-is-for-backward-process/3883/4\n",
    "- https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66\n",
    "- https://discuss.pytorch.org/t/get-forward-and-backward-output-seperately-from-bidirectional-rnn/2523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout, bidirectional):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, bidirectional=bidirectional)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, hidden=None):\n",
    "        \n",
    "        #src = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded, hidden)\n",
    "                \n",
    "        #outputs = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #outputs[-1,  :, :self.hid_dim] is the last of the forwards RNN\n",
    "        #outputs[ 0,  :, self.hid_dim:] is the last of the backwards RNN\n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #therefore:\n",
    "        assert (torch.cat((outputs[-1,:,:self.hid_dim], outputs[0,:,self.hid_dim:]),dim=1) == torch.cat((hidden[-2,:,:], hidden[-1,:,:]),dim=1)).all()\n",
    "        \n",
    "        #we sum, but can take the mean or pass through a linear layer\n",
    "        outputs = outputs[:, :, :self.hid_dim] + outputs[:, :, self.hid_dim:]\n",
    "        hidden = hidden[-2,:,:] + hidden[-1,:,:]\n",
    "        \n",
    "        #outputs = [sent len, batch size, hid dim]\n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.v = nn.Parameter(torch.rand(hid_dim))\n",
    "        \n",
    "        self.rnn = nn.GRU(hid_dim + emb_dim, hid_dim)\n",
    "        \n",
    "        self.out = nn.Linear(hid_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def attention(self, hidden, encoder_outputs):\n",
    "        \n",
    "        #hidden = [batch size, hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, hid dim]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src sent len, hid dim]\n",
    "        #encoder_outputs = [batch size, src sent len, hid dim]\n",
    "        \n",
    "        energy = self.attn(torch.cat((hidden, encoder_outputs),dim=2))\n",
    "        \n",
    "        #energy = [batch size, src sent len, hid dim]\n",
    "        \n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        \n",
    "        #energy = [batch size, hid dim, src sent len]\n",
    "        \n",
    "        #v = [hid dim]\n",
    "        \n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
    "        \n",
    "        #v = [batch size, 1, hid_dim]\n",
    "                \n",
    "        energy = torch.bmm(v, energy).squeeze(1)\n",
    "        \n",
    "        #energy = [batch size, src len]\n",
    "        \n",
    "        return F.softmax(energy, dim=1).unsqueeze(1)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "             \n",
    "        #input = [bsz]\n",
    "        #hidden = [batch size, hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, bsz]\n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, bsz, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        \n",
    "        #a = [bsz, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [bsz, src sent len, hid dim]\n",
    "        \n",
    "        context = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        #context = [bsz, 1, hid dim]\n",
    "        \n",
    "        context = context.permute(1, 0, 2)\n",
    "        \n",
    "        #context = [1, bsz, hid dim]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        \n",
    "        #rnn_input = [1, bsz, hid dim + emb dim]\n",
    "              \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #o = [1, bsz, hid dim]\n",
    "        #h = [n layers, bsz, hid dim]\n",
    "        \n",
    "        output = output.squeeze(0)\n",
    "        context = context.squeeze(0)\n",
    "        \n",
    "        output = self.out(torch.cat((output, context), dim=1))\n",
    "        \n",
    "        #output = [bsz, output dim]\n",
    "        \n",
    "        return output, hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "DROPOUT = 0.5\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, DROPOUT, BIDIRECTIONAL)\n",
    "dec = Decoder(INPUT_DIM, EMB_DIM, HID_DIM, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \n",
    "        #src = [sent len, batch size]\n",
    "        #trg = [sent len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        \n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards summed together\n",
    "        #hidden is the final hidden state of the input sequence, back and forwards summed together\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        output = trg[0,:]\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (trg[t] if teacher_force else top1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        loss = criterion(output[1:].view(-1, output.shape[2]), trg[1:].view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            loss = criterion(output[1:].view(-1, output.shape[2]), trg[1:].view(-1))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/.conda/envs/pytorch04/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 4.588 | Train PPL:  98.301 | Val. Loss: 3.706 | Val. PPL:  40.685 |\n",
      "| Epoch: 02 | Train Loss: 3.593 | Train PPL:  36.336 | Val. Loss: 3.226 | Val. PPL:  25.182 |\n",
      "| Epoch: 03 | Train Loss: 3.151 | Train PPL:  23.353 | Val. Loss: 2.960 | Val. PPL:  19.304 |\n",
      "| Epoch: 04 | Train Loss: 2.858 | Train PPL:  17.421 | Val. Loss: 2.858 | Val. PPL:  17.433 |\n",
      "| Epoch: 05 | Train Loss: 2.653 | Train PPL:  14.201 | Val. Loss: 2.768 | Val. PPL:  15.923 |\n",
      "| Epoch: 06 | Train Loss: 2.456 | Train PPL:  11.657 | Val. Loss: 2.805 | Val. PPL:  16.528 |\n",
      "| Epoch: 07 | Train Loss: 2.323 | Train PPL:  10.208 | Val. Loss: 2.713 | Val. PPL:  15.068 |\n",
      "| Epoch: 08 | Train Loss: 2.191 | Train PPL:   8.944 | Val. Loss: 2.897 | Val. PPL:  18.113 |\n",
      "| Epoch: 09 | Train Loss: 2.082 | Train PPL:   8.019 | Val. Loss: 2.698 | Val. PPL:  14.852 |\n",
      "| Epoch: 10 | Train Loss: 2.005 | Train PPL:   7.424 | Val. Loss: 2.811 | Val. PPL:  16.619 |\n",
      "| Epoch: 11 | Train Loss: 1.921 | Train PPL:   6.826 | Val. Loss: 2.712 | Val. PPL:  15.056 |\n",
      "| Epoch: 12 | Train Loss: 1.851 | Train PPL:   6.364 | Val. Loss: 2.840 | Val. PPL:  17.122 |\n",
      "| Epoch: 13 | Train Loss: 1.809 | Train PPL:   6.105 | Val. Loss: 2.762 | Val. PPL:  15.836 |\n",
      "| Epoch: 14 | Train Loss: 1.752 | Train PPL:   5.764 | Val. Loss: 2.771 | Val. PPL:  15.981 |\n",
      "| Epoch: 15 | Train Loss: 1.718 | Train PPL:   5.576 | Val. Loss: 2.802 | Val. PPL:  16.483 |\n",
      "| Epoch: 16 | Train Loss: 1.684 | Train PPL:   5.389 | Val. Loss: 2.907 | Val. PPL:  18.307 |\n",
      "| Epoch: 17 | Train Loss: 1.614 | Train PPL:   5.022 | Val. Loss: 2.839 | Val. PPL:  17.092 |\n",
      "| Epoch: 18 | Train Loss: 1.605 | Train PPL:   4.978 | Val. Loss: 2.792 | Val. PPL:  16.320 |\n",
      "| Epoch: 19 | Train Loss: 1.573 | Train PPL:   4.822 | Val. Loss: 2.894 | Val. PPL:  18.067 |\n",
      "| Epoch: 20 | Train Loss: 1.553 | Train PPL:   4.723 | Val. Loss: 2.850 | Val. PPL:  17.283 |\n",
      "| Epoch: 21 | Train Loss: 1.523 | Train PPL:   4.587 | Val. Loss: 3.089 | Val. PPL:  21.966 |\n",
      "| Epoch: 22 | Train Loss: 1.463 | Train PPL:   4.321 | Val. Loss: 2.955 | Val. PPL:  19.196 |\n",
      "| Epoch: 23 | Train Loss: 1.458 | Train PPL:   4.297 | Val. Loss: 2.937 | Val. PPL:  18.864 |\n",
      "| Epoch: 24 | Train Loss: 1.443 | Train PPL:   4.235 | Val. Loss: 2.946 | Val. PPL:  19.030 |\n",
      "| Epoch: 25 | Train Loss: 1.403 | Train PPL:   4.067 | Val. Loss: 2.825 | Val. PPL:  16.861 |\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "N_EPOCHS = 25\n",
    "CLIP = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "if not os.path.isdir('.save'):\n",
    "    os.makedirs('.save')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), '.save/tut3_model.pt')\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Val. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/.conda/envs/pytorch04/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 2.628 | Test PPL:  13.841\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('.save/tut3_model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
