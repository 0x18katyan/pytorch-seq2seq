{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/keon/seq2seq/\n",
    "https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
    "https://pytorch.org/docs/stable/nn.html#torch.nn.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.data/multi30k/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Multi30k.download('.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>')\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = TranslationDataset.splits(      \n",
    "  path = '.data/multi30k',  \n",
    "  exts = ['.de', '.en'],   \n",
    "  fields = [('src', SRC), ('trg', TRG)],\n",
    "  train = 'train', \n",
    "  validation = 'val', \n",
    "  test = 'test2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train.src, min_freq=2)\n",
    "TRG.build_vocab(train.trg, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train, valid, test), batch_size=BATCH_SIZE, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://discuss.pytorch.org/t/how-can-i-know-which-part-of-h-n-of-bidirectional-rnn-is-for-backward-process/3883/4\n",
    "- https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66\n",
    "- https://discuss.pytorch.org/t/get-forward-and-backward-output-seperately-from-bidirectional-rnn/2523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, hidden=None):\n",
    "        \n",
    "        #src = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded, hidden)\n",
    "        \n",
    "        #outputs = [sent len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
    "        \n",
    "        self.out = nn.Linear(emb_dim + hid_dim*2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, context):\n",
    "        \n",
    "        #input = [bsz]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #context = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n layers and n directions will both always be 1, therefore:\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        #context = [1, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, bsz]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, bsz, emb dim]\n",
    "                \n",
    "        emb_con = torch.cat((embedded, context), dim=2)\n",
    "            \n",
    "        #emb_con = [1, bsz, emb dim + hid dim]\n",
    "            \n",
    "        output, hidden = self.rnn(emb_con, hidden)\n",
    "        \n",
    "        #outputs = [sent len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #sent len, n layers and n directions will always be 1, therefore:\n",
    "        #outputs = [1, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)),dim=1)\n",
    "        \n",
    "        return self.out(output), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, DROPOUT)\n",
    "dec = Decoder(INPUT_DIM, EMB_DIM, HID_DIM, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \n",
    "        #src = [sent len, batch size]\n",
    "        #trg = [sent len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        \n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is the 'context'\n",
    "        context = self.encoder(src)\n",
    "        \n",
    "        #context also used as the initial hidden state of the decoder\n",
    "        hidden = context\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        output = trg[0,:]\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            output, hidden = self.decoder(output, hidden, context)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (trg[t] if teacher_force else top1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        loss = criterion(output[1:].view(-1, output.shape[2]), trg[1:].view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg)\n",
    "\n",
    "            loss = criterion(output[1:].view(-1, output.shape[2]), trg[1:].view(-1))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/.conda/envs/pytorch04/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 3.784 | Train PPL:  43.974 | Val. Loss: 3.412 | Val. PPL:  30.336 |\n",
      "| Epoch: 02 | Train Loss: 3.324 | Train PPL:  27.767 | Val. Loss: 3.108 | Val. PPL:  22.380 |\n",
      "| Epoch: 03 | Train Loss: 3.030 | Train PPL:  20.700 | Val. Loss: 3.017 | Val. PPL:  20.430 |\n",
      "| Epoch: 04 | Train Loss: 2.804 | Train PPL:  16.515 | Val. Loss: 2.919 | Val. PPL:  18.528 |\n",
      "| Epoch: 05 | Train Loss: 2.607 | Train PPL:  13.560 | Val. Loss: 2.831 | Val. PPL:  16.962 |\n",
      "| Epoch: 06 | Train Loss: 2.458 | Train PPL:  11.678 | Val. Loss: 2.857 | Val. PPL:  17.404 |\n",
      "| Epoch: 07 | Train Loss: 2.324 | Train PPL:  10.217 | Val. Loss: 2.811 | Val. PPL:  16.620 |\n",
      "| Epoch: 08 | Train Loss: 2.205 | Train PPL:   9.071 | Val. Loss: 2.812 | Val. PPL:  16.635 |\n",
      "| Epoch: 09 | Train Loss: 2.105 | Train PPL:   8.207 | Val. Loss: 2.847 | Val. PPL:  17.240 |\n",
      "| Epoch: 10 | Train Loss: 2.033 | Train PPL:   7.635 | Val. Loss: 2.843 | Val. PPL:  17.164 |\n",
      "| Epoch: 11 | Train Loss: 1.959 | Train PPL:   7.089 | Val. Loss: 2.834 | Val. PPL:  17.022 |\n",
      "| Epoch: 12 | Train Loss: 1.898 | Train PPL:   6.673 | Val. Loss: 2.801 | Val. PPL:  16.469 |\n",
      "| Epoch: 13 | Train Loss: 1.843 | Train PPL:   6.317 | Val. Loss: 2.869 | Val. PPL:  17.627 |\n",
      "| Epoch: 14 | Train Loss: 1.802 | Train PPL:   6.061 | Val. Loss: 2.785 | Val. PPL:  16.205 |\n",
      "| Epoch: 15 | Train Loss: 1.744 | Train PPL:   5.719 | Val. Loss: 3.049 | Val. PPL:  21.102 |\n",
      "| Epoch: 16 | Train Loss: 1.699 | Train PPL:   5.468 | Val. Loss: 2.937 | Val. PPL:  18.864 |\n",
      "| Epoch: 17 | Train Loss: 1.660 | Train PPL:   5.259 | Val. Loss: 2.899 | Val. PPL:  18.147 |\n",
      "| Epoch: 18 | Train Loss: 1.666 | Train PPL:   5.289 | Val. Loss: 2.949 | Val. PPL:  19.092 |\n",
      "| Epoch: 19 | Train Loss: 1.593 | Train PPL:   4.920 | Val. Loss: 2.959 | Val. PPL:  19.276 |\n",
      "| Epoch: 20 | Train Loss: 1.546 | Train PPL:   4.694 | Val. Loss: 2.891 | Val. PPL:  18.004 |\n",
      "| Epoch: 21 | Train Loss: 1.532 | Train PPL:   4.629 | Val. Loss: 2.938 | Val. PPL:  18.876 |\n",
      "| Epoch: 22 | Train Loss: 1.508 | Train PPL:   4.518 | Val. Loss: 2.929 | Val. PPL:  18.713 |\n",
      "| Epoch: 23 | Train Loss: 1.499 | Train PPL:   4.475 | Val. Loss: 3.128 | Val. PPL:  22.830 |\n",
      "| Epoch: 24 | Train Loss: 1.467 | Train PPL:   4.337 | Val. Loss: 3.081 | Val. PPL:  21.788 |\n",
      "| Epoch: 25 | Train Loss: 1.444 | Train PPL:   4.237 | Val. Loss: 2.994 | Val. PPL:  19.970 |\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "N_EPOCHS = 25\n",
    "CLIP = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "if not os.path.isdir('.save'):\n",
    "    os.makedirs('.save')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), '.save/tut2_model.pt')\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Val. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/.conda/envs/pytorch04/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 2.940 | Test PPL:  18.908\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('.save/tut2_model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
